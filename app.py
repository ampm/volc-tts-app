import os
import re
import json
import base64
import tempfile
import logging
import uuid
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any

from fastapi import FastAPI, UploadFile, File, HTTPException, Request
from fastapi.responses import HTMLResponse, FileResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from dotenv import load_dotenv

import asyncio
import httpx
from pydub import AudioSegment

# å¯é€‰ï¼šOpenAIï¼Œç”¨äºAIè¯­ä¹‰åˆ†æ®µï¼ˆæ²¡æœ‰Keyä¼šè‡ªåŠ¨å›é€€åˆ°å®‰å…¨åˆ‡åˆ†ï¼‰
from openai import OpenAI

# ========== ENV ==========
load_dotenv()


OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
VOLC_TTS_TOKEN = os.getenv("VOLC_TTS_TOKEN", "")
VOICE_TYPE = os.getenv("VOICE_TYPE", "BV001_streaming")
MAX_LEN = int(os.getenv("MAX_LEN", "300"))
CONCURRENCY = int(os.getenv("CONCURRENCY", "4"))
MAX_RETRIES = int(os.getenv("MAX_RETRIES", "3"))
VOLC_APP_ID = os.getenv("VOLC_APP_ID", "")

LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
def get_volc_tts_token(app_id, access_key_id, secret_access_key):
    url = "https://openspeech.bytedance.com/api/v1/token"
    payload = {
        "app_id": app_id,
        "access_key_id": access_key_id,
        "secret_access_key": secret_access_key,
        "expire_time": 3600
    }
    r = requests.post(url, json=payload, timeout=10)
    r.raise_for_status()
    data = r.json()
    if "token" not in data:
        raise RuntimeError(f"è·å–Tokenå¤±è´¥: {data}")
    return data["token"]

# åœ¨ load_dotenv() åè°ƒç”¨
if not VOLC_TTS_TOKEN:
    VOLC_AK = os.getenv("VOLC_ACCESS_KEY_ID", "")
    VOLC_SK = os.getenv("VOLC_SECRET_ACCESS_KEY", "")
    if not (VOLC_APP_ID and VOLC_AK and VOLC_SK):
        raise RuntimeError("ç¼ºå°‘ VOLC_APP_ID / VOLC_ACCESS_KEY_ID / VOLC_SECRET_ACCESS_KEY")
    VOLC_TTS_TOKEN = get_volc_tts_token(VOLC_APP_ID, VOLC_AK, VOLC_SK)
if not VOLC_TTS_TOKEN:
    raise RuntimeError("ç¯å¢ƒå˜é‡ VOLC_TTS_TOKEN æœªè®¾ç½®")

client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

# ========== LOGGING ==========
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL, logging.INFO),
    format="%(asctime)s [%(levelname)s] %(message)s",
)
logger = logging.getLogger("volc-tts")

def job_log_path(job_id: str) -> Path:
    return OUTPUT_DIR / f"{job_id}.log"

def log_job(job_id: str, message: str):
    """å†™stdout & jobä¸“å±æ—¥å¿—æ–‡ä»¶"""
    logger.info(f"[{job_id}] {message}")
    lp = job_log_path(job_id)
    with lp.open("a", encoding="utf-8") as f:
        f.write(f"{datetime.now().isoformat()} {message}\n")

# ========== PATHS / STATE ==========
OUTPUT_DIR = Path("outputs"); OUTPUT_DIR.mkdir(exist_ok=True)
CHUNKS_ROOT = Path("chunks"); CHUNKS_ROOT.mkdir(exist_ok=True)

JOBS: Dict[str, Dict[str, Any]] = {}
JOBS_LOCK = asyncio.Lock()

app = FastAPI(title="VolcEngine TTS Pipeline â€” Concurrency+Progress+Debug")
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")


# -------- æ–‡æœ¬åˆ‡åˆ† --------
def natural_paragraphs(text: str) -> List[str]:
    parts = [p.strip() for p in re.split(r"\n\s*\n|\r\n\s*\r\n|\n|\r\n", text)]
    return [p for p in parts if p]

def safe_chunks_by_sentence(para: str, max_len: int = 300) -> List[str]:
    sents = re.split(r"([ã€‚ï¼ï¼Ÿï¼›!?\u3002\uFF01\uFF1F\uFF1B])", para)
    sents = [s for s in sents if s and s.strip()]
    chunks, cur = [], ""
    for seg in sents:
        if len(cur) + len(seg) <= max_len:
            cur += seg
        else:
            if cur:
                chunks.append(cur.strip())
            if len(seg) <= max_len:
                cur = seg
            else:
                # å…œåº•ç¡¬åˆ‡ï¼Œæç«¯é•¿å¥
                for i in range(0, len(seg), max_len):
                    piece = seg[i:i+max_len]
                    if i == 0:
                        cur = piece
                    else:
                        chunks.append(cur.strip()); cur = piece
    if cur: chunks.append(cur.strip())
    return chunks

def ai_split_paragraph_local(para: str, max_len: int = 300) -> List[str]:
    if not client:
        return safe_chunks_by_sentence(para, max_len)
    prompt = (
        f"è¯·å°†ä»¥ä¸‹æ–‡æœ¬æŒ‰è¯­ä¹‰è‡ªç„¶åˆ†æ®µï¼Œæ¯æ®µä¸è¶…è¿‡{max_len}ä¸ªæ±‰å­—ï¼›"
        f"ä¸è¦æ‹†å¼€å¥å­ï¼›ä¿æŒåŸæ–‡é¡ºåºï¼›ä»…è¾“å‡ºåˆ†æ®µç»“æœï¼Œæ¯æ®µç‹¬ç«‹ä¸€è¡Œï¼š\n\n{para}"
    )
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
        )
        text = resp.choices[0].message.content.strip()
        lines = [re.sub(r"^\s*(\d+[\.\)ã€ï¼‰])\s*", "", ln).strip() for ln in text.splitlines()]
        lines = [ln for ln in lines if ln]
        fixed = []
        for ln in lines:
            if len(ln) <= max_len:
                fixed.append(ln)
            else:
                fixed.extend(safe_chunks_by_sentence(ln, max_len))
        return fixed or safe_chunks_by_sentence(para, max_len)
    except Exception as e:
        logger.warning(f"AIåˆ†æ®µå¤±è´¥ï¼Œå›é€€å®‰å…¨åˆ‡åˆ†ï¼š{e}")
        return safe_chunks_by_sentence(para, max_len)

def smart_split_full(text: str, max_len: int = 300) -> List[str]:
    chunks: List[str] = []
    for para in natural_paragraphs(text):
        if len(para) <= max_len:
            chunks.append(para)
        else:
            chunks.extend(ai_split_paragraph_local(para, max_len))
    return chunks


# -------- TTSï¼ˆå¹¶å‘+é‡è¯•+æ—¥å¿—ï¼‰ --------
async def volc_tts_async(text: str, idx: int, out_dir: Path, client_http: httpx.AsyncClient, job_id: str) -> Path:
    url = "https://openspeech.bytedance.com/api/v1/tts"
    payload = {
        "app": {
            "appid": VOLC_APP_ID,
            "token": VOLC_TTS_TOKEN,
            "cluster": "volcano_tts",
        },
        "user": {
            "uid": "uid123"
        },
        "audio": {"voice_type": VOICE_TYPE, "encoding": "mp3"},
         "request": {
            "reqid": str(uuid.uuid1()),
            "text": text,
            "operation": "query"
        }
    }
    
    headers = {
        "Authorization": f"Bearer; {VOLC_TTS_TOKEN}", 
        "Content-Type": "application/json",
    }

    delay = 1.0
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            r = await client_http.post(url, headers=headers, json=payload, timeout=60)
            status = r.status_code
            ctype = r.headers.get("Content-Type", "")
            body_preview = (r.text[:200] if r.text else str(r.content[:200]))
            log_job(job_id, f"TTS idx={idx} attempt={attempt} status={status} ctype={ctype} preview={body_preview!r}")

            if status != 200:
                raise RuntimeError(f"http {status}: {body_preview}")

            audio_bytes = None

            if "audio/" in ctype.lower():
                # ç›´æ¥è¿”å›äº†éŸ³é¢‘äºŒè¿›åˆ¶
                audio_bytes = r.content
            else:
                # JSON æˆ– æ–‡æœ¬
                data_obj = None
                if "application/json" in ctype.lower():
                    try:
                        data_obj = r.json()
                    except Exception as e:
                        log_job(job_id, f"TTS idx={idx} json parse failed: {e}. fallback=text")
                        data_obj = None

                audio_b64 = None

                if isinstance(data_obj, dict):
                    # ğŸ”´ å…³é”®åˆ†æ”¯ï¼šdata å¯èƒ½æ˜¯ strï¼ˆç›´æ¥base64ï¼‰æˆ– dictï¼ˆé‡Œè¾¹æ‰æœ‰ audioï¼‰
                    if "data" in data_obj:
                        data_field = data_obj["data"]
                        if isinstance(data_field, str):
                            # ä½ çš„è¿”å›å°±æ˜¯è¿™ç§ï¼šcode=3000, data="<base64>"
                            audio_b64 = data_field
                        elif isinstance(data_field, dict):
                            audio_b64 = (
                                data_field.get("audio")
                                or data_field.get("Audio")
                                or data_field.get("result")
                            )
                    # æ‰“ç‚¹ä¸€ä¸‹ keys æ–¹ä¾¿æ’æŸ¥
                    try:
                        log_job(job_id, f"TTS idx={idx} json keys: {list(data_obj.keys())}")
                    except Exception:
                        pass

                # å¦‚æœè¿˜æ²¡æ‹¿åˆ°ï¼Œå°è¯•æŠŠæ•´ä¸ªæ–‡æœ¬å½“ base64ï¼ˆæœ‰äº›æ¥å£å°±ç›´æ¥å›çº¯æ–‡æœ¬ï¼‰
                if not audio_b64:
                    text_body = (r.text or "").strip().strip('"').strip("'")
                    if len(text_body) > 50 and re.fullmatch(r'[A-Za-z0-9+/=\s]+', text_body or ""):
                        audio_b64 = text_body

                if audio_b64:
                    try:
                        audio_bytes = base64.b64decode(audio_b64)
                    except Exception as e:
                        raise RuntimeError(f"base64 decode failed: {e}. head={audio_b64[:32]}...")

            if not audio_bytes:
                raise RuntimeError("no audio bytes parsed from response")

            fn = out_dir / f"{idx}.mp3"
            fn.write_bytes(audio_bytes)
            return fn

        except Exception as e:
            err_msg = f"TTS idx={idx} attempt={attempt} error: {e}"
            log_job(job_id, err_msg)

            # å¤±è´¥è¯¦æƒ…æ–‡ä»¶
            fail_file = out_dir / f"{idx}_error.json"
            try:
                fail_payload = {
                    "idx": idx,
                    "attempt": attempt,
                    "error": str(e),
                    "response": {
                        "status": status if 'status' in locals() else None,
                        "content_type": ctype if 'ctype' in locals() else None,
                        "body_preview": body_preview if 'body_preview' in locals() else None,
                    },
                    "payload_preview": {
                        "voice_type": VOICE_TYPE,
                        "text_len": len(text),
                    }
                }
                fail_file.write_text(json.dumps(fail_payload, ensure_ascii=False, indent=2), encoding="utf-8")
            except Exception:
                pass

            if attempt >= MAX_RETRIES:
                raise
            await asyncio.sleep(delay)
            delay = min(delay * 2, 8.0)

    raise RuntimeError("unexpected")


def merge_mp3(files: List[Path], output_file: Path) -> None:
    try:
        combined = AudioSegment.empty()
        for f in files:
            combined += AudioSegment.from_file(f, format="mp3")
        combined.export(output_file, format="mp3")
    except FileNotFoundError as e:
        # å¤šåŠæ˜¯ç³»ç»Ÿæœªå®‰è£… ffmpeg/ffprobe
        raise RuntimeError("åˆå¹¶å¤±è´¥ï¼šç³»ç»Ÿç¼ºå°‘ ffmpeg/ffprobeï¼Œè¯·å®‰è£…åé‡è¯•") from e
    except Exception as e:
        raise RuntimeError(f"åˆå¹¶å¤±è´¥ï¼š{e}") from e


# -------- Job çŠ¶æ€ --------
async def set_job(job_id: str, **kwargs):
    async with JOBS_LOCK:
        JOBS.setdefault(job_id, {}).update(kwargs)

async def get_job(job_id: str) -> Dict[str, Any]:
    async with JOBS_LOCK:
        return JOBS.get(job_id, {}).copy()

async def process_job(job_id: str, filename: str, text: str):
    job_dir = CHUNKS_ROOT / job_id
    job_dir.mkdir(parents=True, exist_ok=True)

    log_job(job_id, f"Start process file={filename}, text_len={len(text)}")
    chunks = smart_split_full(text, MAX_LEN)
    total = len(chunks)
    await set_job(job_id, state="splitted", total=total, done=0, failed=0, message="å¼€å§‹TTS...", files=[], failures=[])
    log_job(job_id, f"Splitted chunks={total}")

    sem = asyncio.Semaphore(CONCURRENCY)
    results: List[Path] = [None] * total

    async with httpx.AsyncClient() as http_client:
        async def worker(i: int, content: str):
            idx = i + 1
            try:
                async with sem:
                    fn = await volc_tts_async(content, idx, job_dir, http_client, job_id=job_id)
                results[i] = fn
                st = await get_job(job_id)
                await set_job(job_id, done=st.get("done", 0) + 1)
            except Exception as e:
                st = await get_job(job_id)
                failures = st.get("failures", [])
                failures.append({"idx": idx, "error": str(e)})
                await set_job(job_id, failed=st.get("failed", 0) + 1, failures=failures)
                log_job(job_id, f"FAILED idx={idx}: {e}")

        tasks = [asyncio.create_task(worker(i, c)) for i, c in enumerate(chunks)]
        await asyncio.gather(*tasks)

    await set_job(job_id, message="TTSå®Œæˆï¼Œå¼€å§‹åˆå¹¶...")
    log_job(job_id, "Start merging...")

    ok_files = [p for p in results if p and p.exists()]
    if not ok_files:
        await set_job(job_id, state="error", message="TTSå…¨éƒ¨å¤±è´¥", log=str(job_log_path(job_id)))
        log_job(job_id, "ERROR: all segments failed")
        return

    out_file = OUTPUT_DIR / f"{Path(filename).stem}_{job_id}_final.mp3"
    try:
        merge_mp3(ok_files, out_file)
    except Exception as e:
        await set_job(job_id, state="error", message=str(e), log=str(job_log_path(job_id)))
        log_job(job_id, f"ERROR during merge: {e}")
        return
    
    seg_file = out_file.with_suffix(".segments.txt")
    with seg_file.open("w", encoding="utf-8") as segf:
        for i, c in enumerate(chunks, 1):
            segf.write(f"[{i}]\n{c}\n\n")

    await set_job(job_id, state="done", result=str(out_file), message="å®Œæˆ", log=str(job_log_path(job_id)))
    log_job(job_id, f"Done. output={out_file}")


# -------- é¡µé¢ä¸API --------
@app.get("/", response_class=HTMLResponse)
def index(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/tts")
async def tts(file: UploadFile = File(...)):
    if not file.filename.lower().endswith(".txt"):
        raise HTTPException(status_code=400, detail="è¯·ä¸Šä¼  .txt æ–‡æœ¬æ–‡ä»¶")
    raw = await file.read()
    try:
        text = raw.decode("utf-8")
    except UnicodeDecodeError:
        try:
            text = raw.decode("gbk")
        except Exception:
            raise HTTPException(status_code=400, detail="æ–‡æœ¬ç¼–ç æ— æ³•è¯†åˆ«ï¼Œè¯·ç”¨UTF-8æˆ–GBKã€‚")

    job_id = next(tempfile._get_candidate_names())
    await set_job(job_id, state="queued", total=0, done=0, failed=0, message="æ’é˜Ÿä¸­...")
    log_job(job_id, f"Queued job for filename={file.filename}")
    asyncio.create_task(process_job(job_id, file.filename, text))
    return {"job_id": job_id}

@app.get("/status/{job_id}")
async def status(job_id: str):
    st = await get_job(job_id)
    if not st:
        return JSONResponse({"error": "job_id ä¸å­˜åœ¨"}, status_code=404)
    return st

@app.get("/result/{job_id}")
async def result(job_id: str):
    st = await get_job(job_id)
    if not st:
        raise HTTPException(status_code=404, detail="job_id ä¸å­˜åœ¨")
    if st.get("state") != "done":
        raise HTTPException(status_code=400, detail="ä»»åŠ¡æœªå®Œæˆ")
    return FileResponse(st["result"], media_type="audio/mpeg", filename=Path(st["result"]).name)

# æ–°å¢ï¼šè°ƒè¯•æ¥å£
@app.get("/debug/{job_id}")
async def debug(job_id: str):
    st = await get_job(job_id)
    if not st:
        raise HTTPException(status_code=404, detail="job_id ä¸å­˜åœ¨")

    lp = job_log_path(job_id)
    log_tail = ""
    if lp.exists():
        # åªè¯»æœ€å20KBï¼Œé¿å…å¤ªå¤§
        data = lp.read_text(encoding="utf-8")[-20_000:]
        log_tail = data

    job_dir = CHUNKS_ROOT / job_id
    fail_files = sorted([str(p) for p in job_dir.glob("*_error.json")])

    return {
        "state": st.get("state"),
        "message": st.get("message"),
        "failures": st.get("failures", []),
        "log_file": str(lp),
        "log_tail": log_tail,
        "fail_files": fail_files,
        "output": st.get("result"),
    }
